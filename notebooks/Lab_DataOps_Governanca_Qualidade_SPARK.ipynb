{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Lab DataOps: Governan√ßa e Qualidade de Dados com PySpark e Great Expectations\n",
    "\n",
    "## Objetivos do Laborat√≥rio\n",
    "\n",
    "Neste laborat√≥rio pr√°tico, voc√™ ir√°:\n",
    "- Implementar **testes de qualidade de dados** automatizados com **Great Expectations**\n",
    "- Aplicar as **6 dimens√µes da qualidade** na pr√°tica\n",
    "- Criar um **pipeline DataOps** com valida√ß√µes profissionais\n",
    "- Simular cen√°rios reais de **governan√ßa de dados**\n",
    "- Gerar **relat√≥rios de qualidade** automatizados\n",
    "\n",
    "### Conceitos Aplicados\n",
    "- **DataOps**: Automa√ß√£o e monitoramento cont√≠nuo\n",
    "- **Governan√ßa**: Regras, pol√≠ticas e responsabilidades\n",
    "- **Qualidade**: Acur√°cia, Completude, Consist√™ncia, Pontualidade, Unicidade e Validade\n",
    "- **Great Expectations**: Framework profissional para valida√ß√£o de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configura√ß√£o do Ambiente PySpark e Great Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Great Expectations imports\n",
    "try:\n",
    "    import great_expectations as gx\n",
    "    print(f\"‚úÖ Great Expectations vers√£o: {gx.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Great Expectations n√£o encontrado. Instalando...\")\n",
    "    !pip install great-expectations==0.18.8 sqlalchemy==1.4.46\n",
    "    import great_expectations as gx\n",
    "    print(f\"‚úÖ Great Expectations instalado: {gx.__version__}\")\n",
    "\n",
    "# Inicializar Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataOps_Governanca_Lab_GX\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark Session iniciada: {spark.version}\")\n",
    "print(f\"üìä Contexto: {spark.sparkContext.appName}\")\n",
    "print(f\"üéØ Great Expectations vers√£o: {gx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cria√ß√£o de Dados de Exemplo com PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados simulados com problemas de qualidade intencionais\n",
    "dados_clientes = [\n",
    "    (1, \"Jo√£o Silva\", \"joao@email.com\", \"11999887766\", \"2023-01-15\", \"Ativo\", 25, \"SP\"),\n",
    "    (2, \"Maria Santos\", \"maria.santos@gmail.com\", \"11888776655\", \"2023-02-20\", \"Ativo\", 32, \"RJ\"),\n",
    "    (3, \"Pedro\", \"pedro@invalid\", \"119999\", \"2023-03-10\", \"Inativo\", 150, \"MG\"),  # Problemas: email inv√°lido, telefone incompleto, idade imposs√≠vel\n",
    "    (4, \"Ana Costa\", \"ana@email.com\", \"11777665544\", \"2023-04-05\", \"Ativo\", 28, \"SP\"),\n",
    "    (1, \"Jo√£o Silva\", \"joao@email.com\", \"11999887766\", \"2023-01-15\", \"Ativo\", 25, \"SP\"),  # Duplicata\n",
    "    (5, \"\", \"carlos@email.com\", \"11666554433\", \"2023-05-12\", \"Pendente\", None, \"RS\"),  # Nome vazio, idade nula\n",
    "    (6, \"Lucia Oliveira\", \"lucia@email.com\", \"11555443322\", \"2022-12-01\", \"Ativo\", 45, \"BA\"),\n",
    "    (7, \"Roberto Lima\", \"roberto@email.com\", \"11444332211\", \"2023-06-18\", \"Cancelado\", 38, \"PR\"),  # Status n√£o padr√£o\n",
    "    (8, \"Fernanda\", None, \"11333221100\", \"2023-07-22\", \"Ativo\", 29, \"SC\"),  # Email nulo\n",
    "    (9, \"Marcos Pereira\", \"marcos@email.com\", \"11222110099\", \"2023-08-30\", \"Ativo\", -5, \"GO\")  # Idade negativa\n",
    "]\n",
    "\n",
    "# Schema definido (Governan√ßa: Padr√µes de Dados)\n",
    "schema_clientes = StructType([\n",
    "    StructField(\"id_cliente\", IntegerType(), False),\n",
    "    StructField(\"nome\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"telefone\", StringType(), True),\n",
    "    StructField(\"data_cadastro\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"idade\", IntegerType(), True),\n",
    "    StructField(\"estado\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_clientes_spark = spark.createDataFrame(dados_clientes, schema_clientes)\n",
    "print(\"üìã Dataset de clientes criado com problemas de qualidade intencionais\")\n",
    "df_clientes_spark.show()\n",
    "\n",
    "# Converter para pandas para Great Expectations (evita problemas de serializa√ß√£o)\n",
    "df_clientes_pandas = df_clientes_spark.toPandas()\n",
    "print(f\"üìä Dataset convertido para pandas: {len(df_clientes_pandas)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configura√ß√£o do Great Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar diret√≥rio do Great Expectations\n",
    "project_dir = \"/tmp/gx_lab_dataops\"\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# Inicializar Data Context do Great Expectations\n",
    "try:\n",
    "    context = gx.get_context()\n",
    "    print(\"üìÅ Contexto Great Expectations carregado\")\n",
    "except:\n",
    "    context = gx.get_context(project_root_dir=project_dir)\n",
    "    print(\"üìÅ Novo contexto Great Expectations criado\")\n",
    "\n",
    "print(f\"‚úÖ Great Expectations configurado\")\n",
    "print(f\"üìä Dataset preparado: {len(df_clientes_pandas)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cria√ß√£o de Expectativas de Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar validator usando pandas (evita problemas de serializa√ß√£o do Spark)\n",
    "try:\n",
    "    validator = context.sources.pandas_default.read_dataframe(df_clientes_pandas)\n",
    "    print(\"‚úÖ Validator criado com sucesso\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro: {e}\")\n",
    "    print(\"üîß Tentando m√©todo alternativo...\")\n",
    "    # M√©todo alternativo mais b√°sico\n",
    "    validator = gx.validator.validator.Validator(\n",
    "        execution_engine=gx.execution_engine.PandasExecutionEngine(),\n",
    "        interactive_evaluation=True\n",
    "    )\n",
    "    validator.load_batch_list([gx.core.batch.Batch(data=df_clientes_pandas)])\n",
    "    print(\"‚úÖ Validator criado com m√©todo alternativo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Definindo Expectativas das 6 Dimens√µes da Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ CRIANDO EXPECTATIVAS DAS 6 DIMENS√ïES DA QUALIDADE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. COMPLETUDE (Completeness) - Campos n√£o podem ser nulos\n",
    "print(\"\\nüìä 1. COMPLETUDE - Campos obrigat√≥rios\")\n",
    "validator.expect_column_values_to_not_be_null(\"id_cliente\")\n",
    "validator.expect_column_values_to_not_be_null(\"nome\")\n",
    "validator.expect_column_values_to_not_be_null(\"email\")\n",
    "print(\"   ‚úÖ Expectativas de completude criadas\")\n",
    "\n",
    "# 2. UNICIDADE (Uniqueness) - Valores √∫nicos\n",
    "print(\"\\nüîë 2. UNICIDADE - Chaves √∫nicas\")\n",
    "validator.expect_column_values_to_be_unique(\"id_cliente\")\n",
    "validator.expect_column_values_to_be_unique(\"email\")\n",
    "print(\"   ‚úÖ Expectativas de unicidade criadas\")\n",
    "\n",
    "# 3. VALIDADE (Validity) - Formatos e dom√≠nios\n",
    "print(\"\\n‚úÖ 3. VALIDADE - Formatos e dom√≠nios\")\n",
    "# Email v√°lido\n",
    "validator.expect_column_values_to_match_regex(\n",
    "    \"email\", \n",
    "    r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    ")\n",
    "# Idade v√°lida (0-120)\n",
    "validator.expect_column_values_to_be_between(\"idade\", min_value=0, max_value=120)\n",
    "# Status v√°lido\n",
    "validator.expect_column_values_to_be_in_set(\"status\", [\"Ativo\", \"Inativo\", \"Pendente\"])\n",
    "print(\"   ‚úÖ Expectativas de validade criadas\")\n",
    "\n",
    "# 4. CONSIST√äNCIA (Consistency) - Regras de neg√≥cio\n",
    "print(\"\\nüîÑ 4. CONSIST√äNCIA - Regras de neg√≥cio\")\n",
    "# Telefone deve ter 11 d√≠gitos\n",
    "validator.expect_column_value_lengths_to_equal(\"telefone\", 11)\n",
    "# Data de cadastro n√£o pode ser futura\n",
    "validator.expect_column_values_to_be_dateutil_parseable(\"data_cadastro\")\n",
    "print(\"   ‚úÖ Expectativas de consist√™ncia criadas\")\n",
    "\n",
    "# 5. PONTUALIDADE (Timeliness) - Dados atuais\n",
    "print(\"\\n‚è∞ 5. PONTUALIDADE - Dados atuais\")\n",
    "# Data de cadastro deve ser dos √∫ltimos 2 anos\n",
    "data_limite = (datetime.now() - timedelta(days=730)).strftime(\"%Y-%m-%d\")\n",
    "validator.expect_column_values_to_be_between(\n",
    "    \"data_cadastro\", \n",
    "    min_value=data_limite, \n",
    "    max_value=datetime.now().strftime(\"%Y-%m-%d\")\n",
    ")\n",
    "print(\"   ‚úÖ Expectativas de pontualidade criadas\")\n",
    "\n",
    "# 6. ACUR√ÅCIA (Accuracy) - Dados corretos\n",
    "print(\"\\nüéØ 6. ACUR√ÅCIA - Dados corretos\")\n",
    "# Nome n√£o pode ser vazio\n",
    "validator.expect_column_values_to_not_match_regex(\"nome\", r\"^\\s*$\")\n",
    "# Estado deve ser sigla v√°lida (2 caracteres)\n",
    "validator.expect_column_value_lengths_to_equal(\"estado\", 2)\n",
    "print(\"   ‚úÖ Expectativas de acur√°cia criadas\")\n",
    "\n",
    "print(\"\\nüíæ Todas as expectativas criadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execu√ß√£o das Valida√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar valida√ß√µes\n",
    "print(\"üîç EXECUTANDO VALIDA√á√ïES DE QUALIDADE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validar os dados\n",
    "validation_result = validator.validate()\n",
    "\n",
    "# Analisar resultados\n",
    "print(f\"\\nüìä RESULTADOS DA VALIDA√á√ÉO:\")\n",
    "print(f\"   Total de expectativas: {validation_result.statistics['evaluated_expectations']}\")\n",
    "print(f\"   Expectativas que passaram: {validation_result.statistics['successful_expectations']}\")\n",
    "print(f\"   Expectativas que falharam: {validation_result.statistics['unsuccessful_expectations']}\")\n",
    "print(f\"   Taxa de sucesso: {validation_result.statistics['success_percent']:.1f}%\")\n",
    "\n",
    "# Status geral\n",
    "if validation_result.success:\n",
    "    print(\"\\n‚úÖ VALIDA√á√ÉO PASSOU - Dados dentro dos padr√µes de qualidade\")\n",
    "else:\n",
    "    print(\"\\n‚ùå VALIDA√á√ÉO FALHOU - Problemas de qualidade detectados\")\n",
    "    print(\"\\nüö® PROBLEMAS ENCONTRADOS:\")\n",
    "    \n",
    "    for result in validation_result.results:\n",
    "        if not result.success:\n",
    "            expectation_type = result.expectation_config.expectation_type\n",
    "            column = result.expectation_config.kwargs.get('column', 'N/A')\n",
    "            print(f\"   ‚ùå {expectation_type} - Coluna: {column}\")\n",
    "            if 'partial_unexpected_count' in result.result:\n",
    "                print(f\"      Registros com problema: {result.result['partial_unexpected_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Corre√ß√£o de Dados com PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_dados_qualidade_spark(df_spark):\n",
    "    \"\"\"\n",
    "    Aplica corre√ß√µes usando PySpark baseadas nos resultados das valida√ß√µes\n",
    "    \"\"\"\n",
    "    print(\"üîß APLICANDO CORRE√á√ïES DE QUALIDADE COM PYSPARK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Remover duplicatas (manter primeira ocorr√™ncia)\n",
    "    df_corrigido = df_spark.dropDuplicates([\"id_cliente\"])\n",
    "    print(f\"‚úÖ Duplicatas removidas: {df_spark.count() - df_corrigido.count()} registros\")\n",
    "    \n",
    "    # 2. Corrigir idades inv√°lidas\n",
    "    df_corrigido = df_corrigido.withColumn(\n",
    "        \"idade\",\n",
    "        when((col(\"idade\") < 0) | (col(\"idade\") > 120), None)\n",
    "        .otherwise(col(\"idade\"))\n",
    "    )\n",
    "    print(\"‚úÖ Idades inv√°lidas corrigidas\")\n",
    "    \n",
    "    # 3. Padronizar status\n",
    "    df_corrigido = df_corrigido.withColumn(\n",
    "        \"status\",\n",
    "        when(col(\"status\") == \"Cancelado\", \"Inativo\")\n",
    "        .otherwise(col(\"status\"))\n",
    "    )\n",
    "    print(\"‚úÖ Status padronizados\")\n",
    "    \n",
    "    # 4. Remover registros com campos cr√≠ticos vazios\n",
    "    df_corrigido = df_corrigido.filter(\n",
    "        col(\"nome\").isNotNull() & \n",
    "        (col(\"nome\") != \"\") &\n",
    "        col(\"email\").isNotNull()\n",
    "    )\n",
    "    print(\"‚úÖ Registros com campos cr√≠ticos vazios removidos\")\n",
    "    \n",
    "    print(f\"\\nüìä Registros ap√≥s corre√ß√£o: {df_corrigido.count()}\")\n",
    "    return df_corrigido\n",
    "\n",
    "# Aplicar corre√ß√µes\n",
    "df_clientes_corrigido_spark = corrigir_dados_qualidade_spark(df_clientes_spark)\n",
    "df_clientes_corrigido_pandas = df_clientes_corrigido_spark.toPandas()\n",
    "\n",
    "print(\"\\nüìã Dados ap√≥s corre√ß√£o:\")\n",
    "df_clientes_corrigido_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Re-valida√ß√£o com Dados Corrigidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ RE-VALIDA√á√ÉO COM DADOS CORRIGIDOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar novo validator com dados corrigidos\n",
    "try:\n",
    "    validator_corrigido = context.sources.pandas_default.read_dataframe(df_clientes_corrigido_pandas)\n",
    "    print(\"‚úÖ Novo validator criado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Usando m√©todo alternativo: {str(e)[:50]}...\")\n",
    "    validator_corrigido = gx.validator.validator.Validator(\n",
    "        execution_engine=gx.execution_engine.PandasExecutionEngine(),\n",
    "        interactive_evaluation=True\n",
    "    )\n",
    "    validator_corrigido.load_batch_list([gx.core.batch.Batch(data=df_clientes_corrigido_pandas)])\n",
    "\n",
    "# Aplicar as mesmas expectativas\n",
    "print(\"üéØ Aplicando expectativas aos dados corrigidos...\")\n",
    "\n",
    "# Completude\n",
    "validator_corrigido.expect_column_values_to_not_be_null(\"id_cliente\")\n",
    "validator_corrigido.expect_column_values_to_not_be_null(\"nome\")\n",
    "validator_corrigido.expect_column_values_to_not_be_null(\"email\")\n",
    "\n",
    "# Unicidade\n",
    "validator_corrigido.expect_column_values_to_be_unique(\"id_cliente\")\n",
    "validator_corrigido.expect_column_values_to_be_unique(\"email\")\n",
    "\n",
    "# Validade\n",
    "validator_corrigido.expect_column_values_to_match_regex(\"email\", r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "validator_corrigido.expect_column_values_to_be_between(\"idade\", min_value=0, max_value=120)\n",
    "validator_corrigido.expect_column_values_to_be_in_set(\"status\", [\"Ativo\", \"Inativo\", \"Pendente\"])\n",
    "\n",
    "# Consist√™ncia\n",
    "validator_corrigido.expect_column_value_lengths_to_equal(\"telefone\", 11)\n",
    "validator_corrigido.expect_column_values_to_be_dateutil_parseable(\"data_cadastro\")\n",
    "\n",
    "# Pontualidade\n",
    "data_limite = (datetime.now() - timedelta(days=730)).strftime(\"%Y-%m-%d\")\n",
    "validator_corrigido.expect_column_values_to_be_between(\"data_cadastro\", min_value=data_limite, max_value=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "# Acur√°cia\n",
    "validator_corrigido.expect_column_values_to_not_match_regex(\"nome\", r\"^\\s*$\")\n",
    "validator_corrigido.expect_column_value_lengths_to_equal(\"estado\", 2)\n",
    "\n",
    "# Executar valida√ß√£o\n",
    "validation_result_corrigido = validator_corrigido.validate()\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\nüìä COMPARA√á√ÉO DE RESULTADOS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"DADOS ORIGINAIS:\")\n",
    "print(f\"   Taxa de sucesso: {validation_result.statistics['success_percent']:.1f}%\")\n",
    "print(f\"   Expectativas que falharam: {validation_result.statistics['unsuccessful_expectations']}\")\n",
    "\n",
    "print(f\"\\nDADOS CORRIGIDOS:\")\n",
    "print(f\"   Taxa de sucesso: {validation_result_corrigido.statistics['success_percent']:.1f}%\")\n",
    "print(f\"   Expectativas que falharam: {validation_result_corrigido.statistics['unsuccessful_expectations']}\")\n",
    "\n",
    "if validation_result_corrigido.success:\n",
    "    print(\"\\nüéâ SUCESSO! Dados corrigidos passaram em todas as valida√ß√µes\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Ainda existem problemas nos dados corrigidos\")\n",
    "    \n",
    "melhoria = validation_result_corrigido.statistics['success_percent'] - validation_result.statistics['success_percent']\n",
    "print(f\"\\nüéØ Melhoria alcan√ßada: +{melhoria:.1f} pontos percentuais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise com PySpark - Estat√≠sticas de Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä AN√ÅLISE DE QUALIDADE COM PYSPARK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# An√°lise dos dados originais\n",
    "print(\"\\nüîç DADOS ORIGINAIS:\")\n",
    "print(f\"Total de registros: {df_clientes_spark.count()}\")\n",
    "print(f\"Duplicatas por ID: {df_clientes_spark.count() - df_clientes_spark.dropDuplicates(['id_cliente']).count()}\")\n",
    "print(f\"Valores nulos em 'nome': {df_clientes_spark.filter(col('nome').isNull() | (col('nome') == '')).count()}\")\n",
    "print(f\"Valores nulos em 'email': {df_clientes_spark.filter(col('email').isNull()).count()}\")\n",
    "print(f\"Idades inv√°lidas: {df_clientes_spark.filter((col('idade') < 0) | (col('idade') > 120)).count()}\")\n",
    "\n",
    "# An√°lise dos dados corrigidos\n",
    "print(\"\\n‚úÖ DADOS CORRIGIDOS:\")\n",
    "print(f\"Total de registros: {df_clientes_corrigido_spark.count()}\")\n",
    "print(f\"Duplicatas por ID: {df_clientes_corrigido_spark.count() - df_clientes_corrigido_spark.dropDuplicates(['id_cliente']).count()}\")\n",
    "print(f\"Valores nulos em 'nome': {df_clientes_corrigido_spark.filter(col('nome').isNull() | (col('nome') == '')).count()}\")\n",
    "print(f\"Valores nulos em 'email': {df_clientes_corrigido_spark.filter(col('email').isNull()).count()}\")\n",
    "print(f\"Idades inv√°lidas: {df_clientes_corrigido_spark.filter((col('idade') < 0) | (col('idade') > 120)).count()}\")\n",
    "\n",
    "# Distribui√ß√£o por status\n",
    "print(\"\\nüìà DISTRIBUI√á√ÉO POR STATUS (Dados Corrigidos):\")\n",
    "df_clientes_corrigido_spark.groupBy(\"status\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "# Distribui√ß√£o por estado\n",
    "print(\"\\nüó∫Ô∏è DISTRIBUI√á√ÉO POR ESTADO (Dados Corrigidos):\")\n",
    "df_clientes_corrigido_spark.groupBy(\"estado\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gera√ß√£o de Relat√≥rios de Qualidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar Data Docs (relat√≥rios HTML)\n",
    "print(\"üìà GERANDO RELAT√ìRIOS DE QUALIDADE (DATA DOCS)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Construir Data Docs\n",
    "    context.build_data_docs()\n",
    "    \n",
    "    # Obter URLs dos relat√≥rios\n",
    "    data_docs_sites = context.get_docs_sites_urls()\n",
    "    \n",
    "    print(\"‚úÖ Relat√≥rios de qualidade gerados com sucesso!\")\n",
    "    print(\"\\nüìä RELAT√ìRIOS DISPON√çVEIS:\")\n",
    "    \n",
    "    for site_name, url in data_docs_sites.items():\n",
    "        print(f\"   üìã {site_name}: {url}\")\n",
    "        \n",
    "    print(\"\\nüí° Abra os URLs acima no navegador para visualizar os relat√≥rios detalhados\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao gerar Data Docs: {str(e)}\")\n",
    "\n",
    "# Resumo executivo\n",
    "print(\"\\nüìã RESUMO EXECUTIVO DE QUALIDADE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Dataset Original (PySpark):\")\n",
    "print(f\"   - Registros: {df_clientes_spark.count()}\")\n",
    "print(f\"   - Taxa de qualidade: {validation_result.statistics['success_percent']:.1f}%\")\n",
    "print(f\"   - Status: {'‚úÖ APROVADO' if validation_result.success else '‚ùå REPROVADO'}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Corrigido (PySpark):\")\n",
    "print(f\"   - Registros: {df_clientes_corrigido_spark.count()}\")\n",
    "print(f\"   - Taxa de qualidade: {validation_result_corrigido.statistics['success_percent']:.1f}%\")\n",
    "print(f\"   - Status: {'‚úÖ APROVADO' if validation_result_corrigido.success else '‚ùå REPROVADO'}\")\n",
    "\n",
    "melhoria = validation_result_corrigido.statistics['success_percent'] - validation_result.statistics['success_percent']\n",
    "print(f\"\\nüéØ Melhoria alcan√ßada: +{melhoria:.1f} pontos percentuais\")\n",
    "print(f\"üìâ Redu√ß√£o de registros: {df_clientes_spark.count() - df_clientes_corrigido_spark.count()} registros removidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Pipeline DataOps Completo com PySpark + Great Expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_dataops_pyspark_gx(df_spark, context):\n",
    "    \"\"\"\n",
    "    Pipeline DataOps completo usando PySpark + Great Expectations\n",
    "    \"\"\"\n",
    "    print(\"üöÄ PIPELINE DATAOPS PYSPARK + GREAT EXPECTATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pipeline_result = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"status\": \"EXECUTANDO\",\n",
    "        \"etapas_concluidas\": [],\n",
    "        \"metricas_qualidade\": {},\n",
    "        \"spark_stats\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Etapa 1: An√°lise inicial com PySpark\n",
    "        print(\"\\nüìä Etapa 1: An√°lise inicial com PySpark\")\n",
    "        pipeline_result[\"spark_stats\"][\"registros_originais\"] = df_spark.count()\n",
    "        pipeline_result[\"spark_stats\"][\"duplicatas\"] = df_spark.count() - df_spark.dropDuplicates(['id_cliente']).count()\n",
    "        pipeline_result[\"etapas_concluidas\"].append(\"analise_spark\")\n",
    "        \n",
    "        # Etapa 2: Valida√ß√£o com Great Expectations\n",
    "        print(\"\\nüéØ Etapa 2: Valida√ß√£o com Great Expectations\")\n",
    "        df_pandas = df_spark.toPandas()\n",
    "        validator = context.sources.pandas_default.read_dataframe(df_pandas)\n",
    "        \n",
    "        # Aplicar expectativas\n",
    "        validator.expect_column_values_to_not_be_null(\"id_cliente\")\n",
    "        validator.expect_column_values_to_be_unique(\"id_cliente\")\n",
    "        validator.expect_column_values_to_be_between(\"idade\", min_value=0, max_value=120)\n",
    "        \n",
    "        validation_result = validator.validate()\n",
    "        pipeline_result[\"metricas_qualidade\"][\"inicial\"] = validation_result.statistics\n",
    "        pipeline_result[\"etapas_concluidas\"].append(\"validacao_gx\")\n",
    "        \n",
    "        # Etapa 3: Corre√ß√£o com PySpark (se necess√°rio)\n",
    "        if not validation_result.success:\n",
    "            print(\"\\nüîß Etapa 3: Corre√ß√£o com PySpark\")\n",
    "            df_corrigido = corrigir_dados_qualidade_spark(df_spark)\n",
    "            pipeline_result[\"spark_stats\"][\"registros_corrigidos\"] = df_corrigido.count()\n",
    "            pipeline_result[\"etapas_concluidas\"].append(\"correcao_spark\")\n",
    "            \n",
    "            # Etapa 4: Re-valida√ß√£o\n",
    "            print(\"\\nüîç Etapa 4: Re-valida√ß√£o\")\n",
    "            df_corrigido_pandas = df_corrigido.toPandas()\n",
    "            validator_final = context.sources.pandas_default.read_dataframe(df_corrigido_pandas)\n",
    "            \n",
    "            # Aplicar expectativas novamente\n",
    "            validator_final.expect_column_values_to_not_be_null(\"id_cliente\")\n",
    "            validator_final.expect_column_values_to_be_unique(\"id_cliente\")\n",
    "            validator_final.expect_column_values_to_be_between(\"idade\", min_value=0, max_value=120)\n",
    "            \n",
    "            validation_result_final = validator_final.validate()\n",
    "            pipeline_result[\"metricas_qualidade\"][\"final\"] = validation_result_final.statistics\n",
    "            pipeline_result[\"etapas_concluidas\"].append(\"revalidacao\")\n",
    "            \n",
    "            if validation_result_final.success:\n",
    "                pipeline_result[\"status\"] = \"‚úÖ SUCESSO\"\n",
    "            else:\n",
    "                pipeline_result[\"status\"] = \"‚ö†Ô∏è SUCESSO PARCIAL\"\n",
    "        else:\n",
    "            pipeline_result[\"status\"] = \"‚úÖ SUCESSO\"\n",
    "        \n",
    "        # Etapa 5: Relat√≥rios\n",
    "        print(\"\\nüìà Etapa 5: Gerando relat√≥rios\")\n",
    "        context.build_data_docs()\n",
    "        pipeline_result[\"etapas_concluidas\"].append(\"relatorios\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_result[\"status\"] = \"‚ùå ERRO\"\n",
    "        pipeline_result[\"erro\"] = str(e)\n",
    "    \n",
    "    return pipeline_result\n",
    "\n",
    "# Executar pipeline completo\n",
    "resultado_pipeline = pipeline_dataops_pyspark_gx(df_clientes_spark, context)\n",
    "\n",
    "print(f\"\\nüìã Status final do pipeline: {resultado_pipeline['status']}\")\n",
    "print(f\"üìÖ Timestamp: {resultado_pipeline['timestamp']}\")\n",
    "print(f\"‚úÖ Etapas conclu√≠das: {', '.join(resultado_pipeline['etapas_concluidas'])}\")\n",
    "print(f\"üìä Estat√≠sticas Spark: {resultado_pipeline['spark_stats']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclus√µes\n",
    "\n",
    "### üéØ O que Aprendemos com PySpark + Great Expectations\n",
    "\n",
    "Neste laborat√≥rio, implementamos um **pipeline DataOps profissional** combinando PySpark e Great Expectations:\n",
    "\n",
    "#### ‚úÖ Vantagens da Combina√ß√£o:\n",
    "- **PySpark**: Processamento distribu√≠do e escal√°vel\n",
    "- **Great Expectations**: Valida√ß√µes padronizadas e relat√≥rios\n",
    "- **Integra√ß√£o**: Convers√£o pandas para compatibilidade\n",
    "- **Performance**: Corre√ß√µes em larga escala com Spark\n",
    "- **Governan√ßa**: Controle completo da qualidade\n",
    "\n",
    "#### ‚úÖ 6 Dimens√µes Implementadas:\n",
    "- **Completude**: `expect_column_values_to_not_be_null`\n",
    "- **Unicidade**: `expect_column_values_to_be_unique`\n",
    "- **Validade**: `expect_column_values_to_match_regex`, `expect_column_values_to_be_between`\n",
    "- **Consist√™ncia**: `expect_column_value_lengths_to_equal`\n",
    "- **Pontualidade**: `expect_column_values_to_be_between` (datas)\n",
    "- **Acur√°cia**: `expect_column_values_to_not_match_regex`\n",
    "\n",
    "### üí° Li√ß√µes Principais\n",
    "\n",
    "> **\"PySpark + Great Expectations = Pipeline DataOps escal√°vel e profissional\"**\n",
    "\n",
    "- **Escalabilidade**: PySpark para grandes volumes\n",
    "- **Padroniza√ß√£o**: Great Expectations para valida√ß√µes\n",
    "- **Automa√ß√£o**: Pipeline completo automatizado\n",
    "- **Relat√≥rios**: Data Docs profissionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Limpeza do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalizar Spark Session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark Session finalizada\")\n",
    "print(\"üéì Laborat√≥rio PySpark + Great Expectations conclu√≠do com sucesso!\")\n",
    "print(\"\\nüìö Continue explorando DataOps com PySpark e Great Expectations!\")\n",
    "print(\"\\nüîó Recursos adicionais:\")\n",
    "print(\"   - PySpark: https://spark.apache.org/docs/latest/api/python/\")\n",
    "print(\"   - Great Expectations: https://docs.greatexpectations.io/\")\n",
    "print(\"   - DataOps: https://dataops.live/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}